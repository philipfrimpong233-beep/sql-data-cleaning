# Layoffs Data Cleaning (SQL)

This project is part of my **portfolio** and demonstrates my skills in **SQL-based data cleaning and preprocessing**. The script works on a dataset of global layoffs and shows the step-by-step process of preparing raw data for analysis.

---

## Overview

The `Layoffs.sql` file contains queries focused on cleaning a dataset named `world_layoffs.layoffs`. The script illustrates key data wrangling techniques, including:

* Viewing and exploring the raw dataset
* Identifying duplicate records
* Removing duplicates using CTEs and `ROW_NUMBER()`
* Preparing a clean version of the data for analysis

---

## Requirements

* A SQL environment (MySQL, PostgreSQL, or similar)
* A database named `world_layoffs` containing a table `layoffs` with raw layoff records

---

## How to Use

1. Import the raw layoffs dataset into your SQL database under `world_layoffs.layoffs`.
2. Run the queries in `Layoffs.sql` step by step:

   * Explore the raw data
   * Detect duplicates
   * Apply cleaning queries
3. Confirm that the cleaned table contains unique, consistent records ready for analysis.

---

## Skills Demonstrated

* Data exploration with SQL
* Use of **CTEs** (Common Table Expressions)
* Duplicate detection and removal
* Data quality checks
* SQL best practices for portfolio projects

---

## Portfolio Note

This project is included in my personal portfolio to showcase my **data cleaning and SQL problem-solving abilities**. The goal is to highlight my approach to handling messy datasets and ensuring data reliability for further analysis.# sql-data-cleaning
